{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac912fb",
   "metadata": {},
   "source": [
    "# Multi-Dimensional UMAP Export for Web Viewer\n",
    "\n",
    "This notebook doubles as a tutorial for exporting deterministic 1D/2D/3D embeddings into the Cellucid WebGL viewer bundles.\n",
    "\n",
    "> This variant is pre-configured for the **Braun** dataset; tweak the configuration cell if your files live elsewhere.\n",
    "\n",
    "**In this walkthrough you will**\n",
    "- configure dataset roots once so the same notebook runs on any machine without editing paths elsewhere.\n",
    "- recompute missing UMAP dimensions only when necessary while reusing a single neighbor graph for perfect alignment across 1D/2D/3D.\n",
    "- hydrate metadata + expressions and feed everything into `cellucid.prepare`.\n",
    "- validate the generated binary assets and manifests before handing them to the frontend.\n",
    "\n",
    "Each section calls out why the code exists so you can adapt the pattern to your own datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fc0ed",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "These setup helpers make the notebook location-agnostic: run it from the repo root, from `notebooks/`, or from VS Code and the imports/paths will still resolve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b15986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22b0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "HERE = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root(HERE)\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "    \n",
    "sc.settings.verbosity = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8da4c0",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Keep all project-specific paths and knobs together so rerunning exports becomes a one-cell edit exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72eea0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File locations\n",
    "DATASET_NAME = \"braun\"  # Slug used to name the export directory\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\"  # Base folder shared across raw/experiment data\n",
    "RAW_DIR = DATA_ROOT / \"raw\"  # Location of raw AnnData inputs\n",
    "EXPERIMENT_DIR = DATA_ROOT / \"experiments\"  # Folder containing experiment-specific .h5ad files\n",
    "EXPORT_DIR = Path(\"/Users/kemalinecik/git_nosync/_/cellucid/assets/exports\") / DATASET_NAME  # Final viewer export root\n",
    "\n",
    "# Inputs/outputs\n",
    "EXPERIMENT_FILE = EXPERIMENT_DIR / Path(\"braun_developmental_complete_with_3d_umap.h5ad\")\n",
    "COMPLETE_ADATA_FILE = RAW_DIR / \"braun_developmental_complete.h5ad\"\n",
    "COMPLETE_ADATA_VAR = RAW_DIR / \"dataset_complete_Braun_varnames.pickle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dcec59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs \u00d7 n_vars = 1661498 \u00d7 8192 backed at '/Users/kemalinecik/git_nosync/_/cellucid-data/data/raw/braun_developmental_complete.h5ad'\n",
      "    obs: 'sample_ID', 'organ', 'age', 'cell_type', 'sex', 'sex_inferred', 'concatenated_integration_covariates', 'integration_donor', 'integration_biological_unit', 'integration_sample_status', 'integration_library_platform_coarse', 'n_genes', 'LVL3', 'LVL2', 'LVL1', 'LVL0', '_scvi_batch', '_scvi_labels'\n",
      "    uns: 'metrics', 'neighbors', 'rank_genes_groups', 'umap'\n",
      "    obsm: 'Unintegrated', 'X_pca', 'X_umap', 'harmony', 'scvi'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "LVL0 value counts:\n",
      "  Neuronal_and_Glial: 1614096\n",
      "  Stromal: 19161\n",
      "  Haematopoeitic_lineage: 14788\n",
      "  Endothelial: 11741\n",
      "  Epithelial: 873\n",
      "  Neural_crest: 839\n"
     ]
    }
   ],
   "source": [
    "# Quick peek at the raw AnnData without loading it fully\n",
    "if COMPLETE_ADATA_FILE.exists():\n",
    "    adata_preview = ad.read_h5ad(COMPLETE_ADATA_FILE, backed=\"r\")\n",
    "    print(adata_preview)\n",
    "    if \"LVL0\" in adata_preview.obs:\n",
    "        lvl0_counts = adata_preview.obs[\"LVL0\"].value_counts()\n",
    "        print(\"LVL0 value counts:\")\n",
    "        for label, count in lvl0_counts.items():\n",
    "            print(f\"  {label}: {count}\")\n",
    "    else:\n",
    "        print(\"LVL0 column not found in obs.\")\n",
    "    adata_preview.file.close()\n",
    "    del adata_preview\n",
    "else:\n",
    "    print(f\"Complete AnnData file not found at {COMPLETE_ADATA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7af3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP parameters\n",
    "n_neighbors = 15\n",
    "min_dist = 0.5\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "# Storage keys for each dimensionality we care about.\n",
    "UMAP_DIMENSION_KEYS = {\n",
    "    1: \"X_umap_1d\",\n",
    "    2: \"X_umap_2d\",\n",
    "    3: \"X_umap_3d\",\n",
    "    # 4: \"X_umap_4d\",  # Reserved for future expansion\n",
    "}\n",
    "\n",
    "\n",
    "def compute_umap_embedding(adata_source, n_components: int, min_dist: float, random_state: int) -> np.ndarray:\n",
    "    \"\"\"Compute a UMAP embedding with the provided dimensionality without mutating the source AnnData.\"\"\"\n",
    "    neighbors_params = adata_source.uns.get(\"neighbors\", {}).get(\"params\", {})\n",
    "    use_rep = neighbors_params.get(\"use_rep\", None)\n",
    "\n",
    "    adata_temp = ad.AnnData(\n",
    "        obs=adata_source.obs[[]],\n",
    "        obsp={\n",
    "            \"connectivities\": adata_source.obsp[\"connectivities\"],\n",
    "            \"distances\": adata_source.obsp[\"distances\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if use_rep is not None and use_rep in adata_source.obsm:\n",
    "        adata_temp.obsm[use_rep] = adata_source.obsm[use_rep]\n",
    "\n",
    "    adata_temp.uns[\"neighbors\"] = adata_source.uns[\"neighbors\"].copy()\n",
    "    sc.tl.umap(adata_temp, n_components=n_components, min_dist=min_dist, random_state=random_state)\n",
    "\n",
    "    embedding = adata_temp.obsm[\"X_umap\"].copy()\n",
    "    del adata_temp\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25519cea",
   "metadata": {},
   "source": [
    "## Deterministic Embedding Strategy\n",
    "\n",
    "- **Stable random seed (`RANDOM_SEED`)** keeps layouts reproducible between releases, which is critical when comparing viewer builds, spotting regression diffs, or debugging quantization artifacts.\n",
    "- **Stable kNN graph for all dimensions** means `sc.pp.neighbors` runs once and every 1D/2D/3D embedding encodes the exact same neighbor relationships; cross-dimensional brushing stays intuitive and centroid statistics stay comparable.\n",
    "- **Shared latent representation (`scVI`)** ensures the centroids and connectivities exported later line up with whatever representation was used in training; no silent drift between the viewer and the model.\n",
    "- **Legacy `adata.obsm['X_umap']` alias** mirrors the 3D embedding under the historical key so older ingestion scripts and viewer builds continue to work even though the tutorial now emits explicit multi-dimensional files.\n",
    "- **Backed AnnData checks** let us peek into the `.h5ad` file without loading it fully and skip recomputation when the embeddings are already up to date.\n",
    "\n",
    "Tweak the parameters in the previous cell only when you explicitly want to generate alternative deterministic layouts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee82786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kemalinecik/git_nosync/_/cellucid-data/data/experiments/braun_developmental_complete_with_3d_umap.h5ad does not exist yet. Computing full multi-dimensional embeddings.\n",
      "Computing neighbors once and reusing them for every dimensionality...\n",
      "computing neighbors\n",
      "    finished: added to `.uns['neighbors']`\n",
      "    `.obsp['distances']`, distances for each pair of neighbors\n",
      "    `.obsp['connectivities']`, weighted adjacency matrix (0:02:02)\n",
      "Computing 1D UMAP \u2192 X_umap_1d\n",
      "computing UMAP\n"
     ]
    }
   ],
   "source": [
    "def umap_dimensions_present(exp_file: Path, dim_keys: dict[int, str]) -> tuple[bool, list[int]]:\n",
    "    \"\"\"Return whether each required UMAP embedding is stored in exp_file plus the missing dimensions.\"\"\"\n",
    "    if exp_file is None or not exp_file.exists():\n",
    "        return False, list(dim_keys.keys())\n",
    "\n",
    "    backed = ad.read_h5ad(exp_file, backed=\"r\")\n",
    "    try:\n",
    "        available = set(backed.obsm_keys())\n",
    "    finally:\n",
    "        backed.file.close()\n",
    "    missing = [dim for dim, key in dim_keys.items() if key not in available]\n",
    "    return len(missing) == 0, missing\n",
    "\n",
    "\n",
    "def ensure_umap_embeddings():\n",
    "    \"\"\"Compute multi-dimensional UMAP embeddings only when they are absent on disk.\"\"\"\n",
    "    ready, missing_dims = umap_dimensions_present(EXPERIMENT_FILE, UMAP_DIMENSION_KEYS)\n",
    "\n",
    "    if ready:\n",
    "        print(\n",
    "            f\"\u2713 {EXPERIMENT_FILE.name} already stores \"\n",
    "            f\"{', '.join(f'{dim}D' for dim in UMAP_DIMENSION_KEYS)} embeddings.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    missing_msg = \", \".join(f\"{dim}D\" for dim in missing_dims) if missing_dims else \"all required\"\n",
    "    if EXPERIMENT_FILE.exists():\n",
    "        print(f\"Updating {EXPERIMENT_FILE.name}: missing {missing_msg} embeddings.\")\n",
    "    else:\n",
    "        print(f\"{EXPERIMENT_FILE} does not exist yet. Computing full multi-dimensional embeddings.\")\n",
    "\n",
    "    if not COMPLETE_ADATA_FILE.exists():\n",
    "        raise FileNotFoundError(f\"Complete AnnData file not found at {COMPLETE_ADATA_FILE}\")\n",
    "\n",
    "    adata = ad.read_h5ad(COMPLETE_ADATA_FILE)\n",
    "    print(\"Computing neighbors once and reusing them for every dimensionality...\")\n",
    "    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=RANDOM_SEED, use_rep=\"scvi\")\n",
    "\n",
    "    for n_dim, key in UMAP_DIMENSION_KEYS.items():\n",
    "        print(f\"Computing {n_dim}D UMAP \u2192 {key}\")\n",
    "        adata.obsm[key] = compute_umap_embedding(\n",
    "            adata, n_components=n_dim, min_dist=min_dist, random_state=RANDOM_SEED\n",
    "        )\n",
    "\n",
    "    if 3 in UMAP_DIMENSION_KEYS:\n",
    "        # Keep a 3D copy under \"X_umap\" because legacy viewer builds still expect this key\n",
    "        # even though newer ones read the explicit multi-dimensional files.\n",
    "        adata.obsm[\"X_umap\"] = adata.obsm[UMAP_DIMENSION_KEYS[3]].copy()\n",
    "\n",
    "    print(\"UMAP embeddings computed:\")\n",
    "    for n_dim, key in UMAP_DIMENSION_KEYS.items():\n",
    "        shape = adata.obsm[key].shape\n",
    "        print(f\"  {key}: {shape}\")\n",
    "\n",
    "    adata.write_h5ad(EXPERIMENT_FILE)\n",
    "    print(f\"Saved updated embeddings to {EXPERIMENT_FILE}\")\n",
    "\n",
    "    del adata\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "ensure_umap_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028629e",
   "metadata": {},
   "source": [
    "## Load UMAP run\n",
    "\n",
    "The previous step guarantees that the experiment file exists and houses every required UMAP dimension. Load it now and double-check which embeddings are present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_FILE is None or not EXPERIMENT_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"UMAP file not found. Set EXPERIMENT_FILE to a valid .h5ad under {EXPERIMENT_DIR}\"\n",
    "    )\n",
    "\n",
    "adata = ad.read_h5ad(EXPERIMENT_FILE)\n",
    "if \"age\" in adata.obs:\n",
    "    adata.obs[\"age\"] = pd.to_numeric(adata.obs[\"age\"], errors=\"coerce\")\n",
    "drop_columns = [col for col in (\"_scvi_batch\", \"_scvi_labels\") if col in adata.obs]\n",
    "if drop_columns:\n",
    "    adata.obs = adata.obs.drop(columns=drop_columns)\n",
    "adata.var = pd.read_pickle(COMPLETE_ADATA_VAR)\n",
    "\n",
    "legacy_umap_key = \"X_umap\"  # 3D default used by legacy exports/viewers\n",
    "available_umaps = {}\n",
    "for dim, key in UMAP_DIMENSION_KEYS.items():\n",
    "    dim_label = f\"{dim}d\"\n",
    "    if key in adata.obsm:\n",
    "        available_umaps[dim_label] = adata.obsm[key]\n",
    "        print(f\"\u2713 Found {key}: shape {adata.obsm[key].shape}\")\n",
    "    else:\n",
    "        print(f\"\u2717 Missing {key}\")\n",
    "\n",
    "if not available_umaps:\n",
    "    if legacy_umap_key in adata.obsm:\n",
    "        print(f\"Using legacy {legacy_umap_key}\")\n",
    "        available_umaps['3d'] = adata.obsm[legacy_umap_key]\n",
    "    else:\n",
    "        raise KeyError(f\"No UMAP embeddings found in adata.obsm\")\n",
    "\n",
    "print(f\"\\nAvailable dimensions: {list(available_umaps.keys())}\")\n",
    "adata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60b52b",
   "metadata": {},
   "source": [
    "## Quick UMAP stats\n",
    "\n",
    "Lightweight sanity check on all loaded UMAP embeddings (1D, 2D, 3D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats for all available UMAP dimensions\n",
    "umap_stats = {}\n",
    "for dim, coords in available_umaps.items():\n",
    "    umap_stats[dim] = {\n",
    "        \"shape\": coords.shape,\n",
    "        \"mean\": coords.mean(axis=0).tolist(),\n",
    "        \"std\": coords.std(axis=0).tolist(),\n",
    "        \"min\": coords.min(axis=0).tolist(),\n",
    "        \"max\": coords.max(axis=0).tolist(),\n",
    "    }\n",
    "\n",
    "print(f\"UMAP stats for {adata.n_obs} cells:\")\n",
    "for dim, stats in umap_stats.items():\n",
    "    print(f\"\\n{dim.upper()}:\")\n",
    "    print(f\"  Shape: {stats['shape']}\")\n",
    "    print(f\"  Mean: {[f'{x:.3f}' for x in stats['mean']]}\")\n",
    "    print(f\"  Std:  {[f'{x:.3f}' for x in stats['std']]}\")\n",
    "\n",
    "umap_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7c0c1",
   "metadata": {},
   "source": [
    "## Load full annotated data\n",
    "\n",
    "The export step needs the full expression + metadata matrices, not just the subset used for UMAP projection. We therefore:\n",
    "- read the complete AnnData object,\n",
    "- align it to the experiment cell ordering, and\n",
    "- apply a lightweight normalize/log1p transform so the quantized export stays well behaved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not COMPLETE_ADATA_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Complete AnnData file not found at {COMPLETE_ADATA_FILE}\")\n",
    "\n",
    "adata_complete = ad.read_h5ad(COMPLETE_ADATA_FILE)\n",
    "# adata_complete = adata_complete[:, adata_complete.var[\"highly_variable\"] == 1]\n",
    "adata_complete = adata_complete[adata.obs.index].copy()\n",
    "\n",
    "# Normalize counts to 1e4 per cell and log-transform for export\n",
    "sc.pp.normalize_total(adata_complete, target_sum=1e4)\n",
    "sc.pp.log1p(adata_complete)\n",
    "\n",
    "adata_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082731bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "row = adata_complete.X[0]\n",
    "dense_row = row.A.ravel() if hasattr(row, \"A\") else np.asarray(row).ravel()\n",
    "non_zero_preview = dense_row[dense_row != 0][:5]\n",
    "non_zero_preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1740e94",
   "metadata": {},
   "source": [
    "## Export for web viewer\n",
    "\n",
    "`cellucid.prepare` handles the heavy lifting described in `src/cellucid/prepare_data.py`:\n",
    "- quantizes continuous obs/var fields and expression matrices to keep payloads small,\n",
    "- auto-picks compact categorical dtypes and gzips the resulting binaries, and\n",
    "- emits dataset manifests (`dataset_identity.json`, `obs_manifest.json`, `var_manifest.json`) that the WebGL viewer reads at runtime.\n",
    "\n",
    "The call below wires our multi-dimensional UMAPs plus metadata into that exporter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6de24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cellucid import prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8584a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare(\n",
    "    # Multi-dimensional UMAP embeddings; missing keys evaluate to None and will be skipped\n",
    "    X_umap_1d=available_umaps.get('1d'),\n",
    "    X_umap_2d=available_umaps.get('2d'),\n",
    "    X_umap_3d=available_umaps.get('3d'),\n",
    "    # X_umap_4d is reserved for future development\n",
    "\n",
    "    # Other data matrices (scVI latent space drives centroids/kNN reuse)\n",
    "    latent_space=adata.obsm[\"scvi\"],\n",
    "    obs=adata.obs,\n",
    "    var=adata.var,\n",
    "    gene_expression=adata.X,\n",
    "    connectivities=adata.obsp['connectivities'],\n",
    "\n",
    "    # Export behavior knobs defined inline for clarity\n",
    "    var_gene_id_column=None,  # Use var.index as gene identifiers\n",
    "    gene_identifiers=None,  # Export every gene; slice list here if needed\n",
    "    centroid_outlier_quantile=0.90,  # Trim cells far from centroid when summarizing categories\n",
    "    centroid_min_points=10,  # Require at least this many cells per centroid\n",
    "    force=False,\n",
    "    var_quantization=8,\n",
    "    obs_continuous_quantization=8,\n",
    "    obs_categorical_dtype=\"auto\",\n",
    "    compression=6,\n",
    "\n",
    "    # Dataset identity metadata surfaced in dataset_identity.json\n",
    "    out_dir=EXPORT_DIR,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    dataset_description=\"Braun developmental atlas\",\n",
    "    source_name=\"Braun et al.\",\n",
    "    source_url=\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ceebe5",
   "metadata": {},
   "source": [
    "## Validate export artifacts\n",
    "\n",
    "Spot-check file sizes (MB), manifest stats, and total obs/var directory sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19552d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "BYTES_IN_MB = 1024 * 1024\n",
    "\n",
    "def size_mb(path: Path) -> float:\n",
    "    return round(path.stat().st_size / BYTES_IN_MB, 3) if path.exists() else 0\n",
    "\n",
    "def dir_stats(path: Path) -> dict:\n",
    "    if not path.exists():\n",
    "        return {\"size_mb\": 0, \"files\": 0}\n",
    "    total_bytes = 0\n",
    "    file_count = 0\n",
    "    for p in path.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            file_count += 1\n",
    "            total_bytes += p.stat().st_size\n",
    "    return {\"size_mb\": round(total_bytes / BYTES_IN_MB, 3), \"files\": file_count}\n",
    "\n",
    "# Multi-dimensional point files\n",
    "points_files = {\n",
    "    'points_1d': EXPORT_DIR / \"points_1d.bin.gz\",\n",
    "    'points_2d': EXPORT_DIR / \"points_2d.bin.gz\",\n",
    "    'points_3d': EXPORT_DIR / \"points_3d.bin.gz\",\n",
    "    'points (legacy)': EXPORT_DIR / \"points.bin.gz\",\n",
    "}\n",
    "\n",
    "obs_manifest_path = EXPORT_DIR / \"obs_manifest.json\"\n",
    "var_manifest_path = EXPORT_DIR / \"var_manifest.json\"\n",
    "dataset_identity_path = EXPORT_DIR / \"dataset_identity.json\"\n",
    "obs_dir = EXPORT_DIR / \"obs\"\n",
    "var_dir = EXPORT_DIR / \"var\"\n",
    "\n",
    "obs_manifest = json.loads(obs_manifest_path.read_text()) if obs_manifest_path.exists() else None\n",
    "var_manifest = json.loads(var_manifest_path.read_text()) if var_manifest_path.exists() else None\n",
    "dataset_identity = json.loads(dataset_identity_path.read_text()) if dataset_identity_path.exists() else None\n",
    "\n",
    "# Check which point files exist\n",
    "point_sizes = {}\n",
    "for name, path in points_files.items():\n",
    "    if path.exists():\n",
    "        point_sizes[name] = size_mb(path)\n",
    "        print(f\"\u2713 {name}: {point_sizes[name]} MB\")\n",
    "    else:\n",
    "        print(f\"\u2717 {name}: not found\")\n",
    "\n",
    "# Show embeddings metadata\n",
    "if dataset_identity and 'embeddings' in dataset_identity:\n",
    "    embeddings_meta = dataset_identity['embeddings']\n",
    "    print(f\"\\nEmbeddings metadata:\")\n",
    "    print(f\"  Available dimensions: {embeddings_meta.get('available_dimensions')}\")\n",
    "    print(f\"  Default dimension: {embeddings_meta.get('default_dimension')}D\")\n",
    "\n",
    "{\n",
    "    \"paths\": {\n",
    "        \"export_dir\": EXPORT_DIR,\n",
    "        \"obs_manifest\": obs_manifest_path,\n",
    "        \"var_manifest\": var_manifest_path,\n",
    "        \"dataset_identity\": dataset_identity_path,\n",
    "        \"obs_dir\": obs_dir,\n",
    "        \"var_dir\": var_dir,\n",
    "    },\n",
    "    \"sizes_mb\": {\n",
    "        **point_sizes,\n",
    "        \"obs_manifest\": size_mb(obs_manifest_path),\n",
    "        \"var_manifest\": size_mb(var_manifest_path),\n",
    "        \"dataset_identity\": size_mb(dataset_identity_path),\n",
    "    },\n",
    "    \"dir_sizes_mb\": {\n",
    "        \"obs\": dir_stats(obs_dir),\n",
    "        \"var\": dir_stats(var_dir),\n",
    "    },\n",
    "    \"manifest_stats\": {\n",
    "        \"obs\": None if obs_manifest is None else {\n",
    "            \"n_points\": obs_manifest.get(\"n_points\"),\n",
    "            \"fields\": len(obs_manifest.get(\"fields\", [])),\n",
    "            \"centroid_outlier_quantile\": obs_manifest.get(\"centroid_outlier_quantile\"),\n",
    "        },\n",
    "        \"var\": None if var_manifest is None else {\n",
    "            \"n_points\": var_manifest.get(\"n_points\"),\n",
    "            \"fields\": len(var_manifest.get(\"fields\", [])),\n",
    "            \"var_gene_id_column\": var_manifest.get(\"var_gene_id_column\"),\n",
    "        },\n",
    "        \"embeddings\": None if dataset_identity is None else dataset_identity.get(\"embeddings\"),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a61ef",
   "metadata": {},
   "source": [
    "Done. Serve `index.html` from the repo root to view the exported data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tardis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}